{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGtor: Supplimentary Synthetic Image Generetion for Object Detection and Segmentation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the challenges in deep learning tasks, whether it is classification, detection or segmentation, is the need for huge collection of well-balanced training data. Especially, this challenge becomes serious when it comes to the latter two tasks as creating large training dataset for these tasks is a dauntingly tedious and error-prone process. As a result, data augmentation has become an indispensable part of training deep learning models so that the small datasets one has acquired gets multiplied with various morphological or geometrical tweaking, either on fly during training process or offline.\n",
    "\n",
    "Here, I want to show how it is possible to artificially generate, theoretically an infinite number of, new supplementary artificial datasets for object detection or segmentation challenge from a given existing small(in fact the size doesn’t matter as long as one needs more) dataset. The algorithm I am about to share is a simple copy-paste based augmentation but very robust in handling object overlapping, dynamic placement on a given background image and supports object level individual augmentation as well as image wide augmentation. The synthetically generated images will have instance segmentation masks and tightly fit bounding boxes. Let us (me &#x1F60F;) call the overall system SIGtor, to mean Synthetic-Image-Generator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "<ol>\n",
    "<li>\n",
    "You have a dataset you wanted to artificially extend using SIGtor. <b><i>Note that</i></b> SIGtor is an offline dataset generator not an augmentation technique you use while training a deep learning model. The generated images, however, with or without the original images can be used to train a model.\n",
    "</li>\n",
    "<li>\n",
    "Your dataset should be annotated in YOLO formant with bounding boxes annotated as: <ul> <i>./Datasets/Source/Images/image1.jpg $x_1$,$y_1$,$x_2$,$y_2$,$A$ $x_1$,$y_1$,$x_2$,$y_2$,$B$ $x_1$,$y_1$,$x_2$,$y_2$,$C$ $x_1$,$y_1$,$x_2$,$y_2$,$D$</i> </ul>\n",
    "<br>\n",
    "For this demo I will use Pascal VOC or COCO Object Detection and Instance Segmentation Mask downloaded from Kaggle or COCO dataset site. The tools to convert either Pascal VOC or COCO dataset into YOLO format is found in tools folder of this project.\n",
    "\n",
    "Though may not necessarily be in the project folder, one can consider arranging folders as shown in the figure below.\n",
    "\n",
    "<br>\n",
    "<ul>\n",
    "<img src=\"./misc/images/folder_hirearchy.jpg\">\n",
    "<br>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "Download some images from the internet that can be used as a background image and keep it under BackgroundImages folder as shown in the figure above. (Where you put it is up to you as long as you gave the right directory in the project). The background images are not mandatory for the project but is good to create realistic and real world looking artificial images rather than using a plain background. There are many ways to automate the download process such as <a href=\"https://github.com/hardikvasa/google-images-download\"> here </a> or <a href=\"https://levelup.gitconnected.com/how-to-download-google-images-using-python-2021-82e69c637d59\"> here</a>. Once the background images are downloaded, manually remove background images that have objects from your datasets classes. (This is an essential step as you might not want unannotated objects in your training dataset confusing your model’s loss functions).\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SIGtor: The Steps\n",
    "\n",
    "Synthetic image generation has two steps.\n",
    "<ul>\n",
    "<div>\n",
    "<li> Step 1: Expand the source annotation. </li>\n",
    "<center><img src=\"./misc/images/example1.png\" width=\"500\" /></center>\n",
    "\n",
    "\n",
    "Take for example the above image has four objects (A, B, C and D) annotated as:\n",
    "\n",
    "\n",
    "./Datasets/Source/Images/image1.jpg $x_1$,$y_1$,$x_2$,$y_2$,$A$ $x_1$,$y_1$,$x_2$,$y_2$,$B$ $x_1$,$y_1$,$x_2$,$y_2$,$C$ $x_1$,$y_1$,$x_2$,$y_2$,$D$\n",
    "\n",
    "<i>(Of course A, B, C, D will be the object classes integer index and the coordinates will be different according to the objects actual coordinates!)</i>\n",
    "\n",
    "\n",
    "The expand_annotation.py file will take in such annotations, and automatically calculates the IoU of each object against each other, re-annotates the line into several annotation lines so that:\n",
    "<div>\n",
    "<ul>\n",
    "<li> non-overlapping objects gets their own annotation line, like the case with object <b>D<b>.</li>\n",
    "<li> object completely embedded within coordinate of other bigger object (e.g. object <b>B</b> completely within the coordinate of <b>A</b>), also gets its own annotation line.</li>\n",
    "<li> bigger objects with other inner objects (e.g. <b>A</b> and <b>B</b>'s relationship) or partially overlapping with other objects (e.g. <b>A</b> and <b>C</b>'s relationship) should be annotated in the same line.\n",
    "\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Therefore, after expanding the above annotation line will have at least three annotation lines as in below.\n",
    "\n",
    "<div>\n",
    "./Datasets/Source/Images/image1.jpg $x_1$,$y_1$,$x_2$,$y_2$,$B$<br>\n",
    "./Datasets/Source/Images/image1.jpg $x_1$,$y_1$,$x_2$,$y_2$,$D$<br>\n",
    "./Datasets/Source/Images/image1.jpg $x_1$,$y_1$,$x_2$,$y_2$,$A$ $x_1$,$y_1$,$x_2$,$y_2$,$B$ $x_1$,$y_1$,$x_2$,$y_2$,$C$<br>\n",
    "</div>\n",
    "\n",
    "To accomplish this first step, we simply run the expand_annotation.py file with command line arguments or without, given that we edited the sig_argument.txt file with the correct inputs.\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<li> Step 2: Generate the artificial images. </li>\n",
    "\n",
    "The detail of this ... I leave it to the below gif &#x1F62B; phew.\n",
    "<center> <img src=\"./misc/images/SIGtor.gif\" width=\"900\"></center>\n",
    "</div>\n",
    "</ul>\n",
    "\n",
    "Some sample SIGtored images and masks are found in projects Datasets/SIGtored folder. To generate new artificial images one can, clone this project and run synthetic_image_generator.py as it is. To work on your own dataset or other public datasets like COCO and VOC for your next object detection or segmentation training, edit sig_argument.txt file accordingly and follow the above two steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "This experimental project has helped me train YOLOv3 and my own version of YOLOv3 called MultiGridDet and DenseYOLO, my lighter version of YOLOv2 implementation, to get few extra percentages of accuracies compared to the original work of the authors of YOLO and has lead me to believe that copy-paste augmentation is really phenomenal for training deep learning models. However,\n",
    "\n",
    "<ol>\n",
    "<li> One must pay attention not to over represent certain set of object classes. Though I removed part of the project that under-samples over-represented classes such as Person and Car to even out or reduce the imbalance, one is free to experiment with such features.</li>\n",
    "<li> One must make sure that the training dataset is NOT too repetitive resulting in an overfitting problem.</li>\n",
    "</ol>\n",
    "\n",
    "Finally, I attest that I haven't observed an impact or (significant scale of impact) on the performance of the models I trained due to the remaining artifacts of the SIGtored objects or lack of somewhat seamlessness of the pasting. However, the longer you train the model the more easily the network starts to pick those objects, but again that is always the case when your model learns too many details of your training dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
